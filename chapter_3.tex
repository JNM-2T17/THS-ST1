%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   Filename    : chapter 3.tex 
%
%   Description : This file will contain your Theoretical Framework
%                 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Theoretical Framework}
\label{sec:theoframe}

This chapter discusses the algorithms, formulas, and existing procedures with regards to our study. This chapter is divided into four sections. The first section discusses data collection libraries. The second section discusses algorithms for community detection. The third section discusses algorithms for sentiment analysis and other similarity parameters used for community detection. The fourth section discusses community evaluation metrics.

\section{Data Collection}



\section{Community Detection}

\citeA{Clauset:2004} presents an improvement to community detection algorithms. Other existing algorithms presented in this paper are correct logically, but are computationally expensive and cannot run over extremely large datasets in a reasonable amount of time. This paper presents an algorithm that is identical in terms of output but is significantly faster than existing algorithms in terms of runtime.

The algorithm is based on the greedy optimization of modularity, which is a property that indicates the strength of division of a network into communities. The higher the modularity, the better the community structure. Given a graph with $n$ vertices and $m$ edges, the algorithm defines two quantities, namely:

\begin{equation}
	e_{ij} = \frac{1}{2m} \sum_{vw}{A_{vw}\delta(c_v,i)\delta(c_w,i)}
\end{equation}which is the fraction of edges that join vertices in community $i$ to vertices in community $j$, and

\begin{equation}
	a_i = \frac{1}{2m} \sum_{v}{k_v\delta(c_v,i)}
\end{equation}which is the fraction of ends of edges that are attached to vertices in community $i$. The algorithm then defines the modularity $Q$ as:

\begin{equation}
	Q = \sum_{i}({e_{ii}-a_i^2})
\end{equation}

The algorithm finds the largest $Q$ that would result from merging two arbitrary communities, and merging those two communities. The algorithm's main offering is that it skips calculating $Q$ when the two communities have no edges between them. offering an increase in performance. The algorithm maintains a matrix $\Delta Q_{ij}$ for each pair i, j of communities with at least one edge between them, a max-heap $H$ containing the largest $Q$ for each row of the matrix, and a vector array with elements $a_{i}$. $\Delta Q_{ij}$ is defined as follows:

\begin{equation}
	\Delta Q_{ij} =
	\begin {cases}
	\frac{1}{2m}-\frac{k_ik_j}{(2m)^2} &\text{if }i, j\text{ are connected,}
	\\ 0 & \text{otherwise}
\end{cases}
\end{equation}

and a as:

\begin{equation}
	a_i = \frac{k_i}{2m}
\end{equation}

The algorithm runs as follows:
(1) Calculate the initial values of $\Delta Q_{ij}$ and $a_{i}$, and populate the max-heap with the largest element of each row of the matrix $\Delta Q$.
(2) Select the largest $\Delta Q_{ij}$ from H, join the corresponding
communities, update the matrix $\Delta Q$, the heap H, and $a_{i}$, and increment Q by $\Delta Q_{ij}$.
(3) Repeat step 2 until only one community remains.

The algorithm was run against purchase data from amazon.com. The data graph worked on was quite big, with 409,687 items and 2,464,630 edges. The algorithm was able to successfully structure the data into communities based mainly on purchasing information. The proponents were successful in discovering clear communities that correspond to specific topics or genres of books or music, indicating that the purchasing tendencies of Amazon customers are strongly correlated with subject matter.

The proponents hope that this algorithm will allow datasets with millions of vertices and tens of millions of edges to be processed using current computing resources in an efficient manner \cite{Clauset:2004}.

\citeA{Tang:2010} wrote a textbook on community detection where they introduced characteristics of social media, reviewed representative tasks of computing with social media, and illustrated associated challenges because with the emergence of social media websites, they felt it was an avenue to study human interaction and collaboration on an unparalleled scale. Multiple community detection approaches were discussed such as node-centric, network-centric, and hierarchy-centric. 

Node-centric algorithms involve the maximum clique detection problem which involves searching for a maximum complete subgraph of nodes in a network graph that are all adjacent to each other. The clique percolation method can find overlapping communities by finding cliques of size $k$, and then producing a clique graph, wherein two cliques are connected if they share $k-1$ nodes. Each connected element in this clique graph is then a community.

Network-centric algorithms involves vertex similarity, which is the similarity of the node`s social circles or how many common friends the two nodes have. This is what structural equivalence deals with. Two nodes $v_i$ and $v_j$ are structurally equivalent if $\forall v_k \in \{x \mid x \neq v_i \wedge x \neq v_j \}$ $e(v_i,v_k) \in E \iff e(v_j,v_k) \in E$, where $E$ is the set of vertices in the graph, which is to say, $v_i$ and $v_j$ are connected to the exact same nodes. Nodes that are structurally equivalent belong to a community. 

Hierarchy-centric algorithms come in two forms: divisive and agglomerative. In divisive clustering, the entire set of nodes starts out in one set and each time, each set is divided into two until each community only has one member. The division is done by finding the node with the lowest edge betweenness and removing it, since that node is most likely the node connecting two communities. Agglomerative clustering starts with each node in their own community and communities are joined if they increase the overall modularity of the set of communities. Modularity is given by

\begin{equation}
	Q = \frac{1}{2m} \sum_{l = 1}^{k} \sum_{i \in C_l, j \in C_l} (A_{ij} - \frac{d_i d_j}{2m})
\end{equation}where $m$ is the number of edges, $d_i$ is the degree of node $v_i$, $C_l$ being the $lth$ community, and $A_{ij}$ being the value in the adjacency matrix for node $v_i$ and $v_j$.

These algorithms may be considered in the final community detection phase of the proposed research \cite{Tang:2010}.

\citeA{Lim:2012:1} aimed to detect communities that share common interests on Twitter, based on linkages among followers of celebrities representing an interest category because they wish to help markets identify target groups with common interests. However, their approach differs from the typical  paradigm of ``identify communities then, for each, identify interests'', for they first identified interests they wish to extract communities from and from these interests, they then extracted the communities. 

Given a set of celebrities, C, celebrities being users with more than ten thousand followers, the algorithm first gets the common followers of all the celebrities in the set using the formula. 

\begin{equation}
	P = \bigcap_{j \in C} (\bigcup_i link(i,c_j))
\end{equation}where $link(i,j)$ is given by

\begin{equation}
	link(i,j) = \begin{cases}
		\{i\} & \text{i follows j} \\
		\emptyset & \text{i does not follow j}
	\end{cases}
\end{equation}

Given this set of fans, P, they used the Infomap Algorithm and the Clique Percolation Method to detect communities in P. For each interest they wish to extract communities for, they chose the top 6 most popular celebrities based on follower count. Google and Wikipedia were used to identify which interests a celebrity represents. Afterwards, all users that follow the 6 celebrities were selected. They then selected 200,858 random users as a control group. Their algorithm produced more communities and larger communities than the control group, as well as more consistent communities, having a higher clustering coefficient. 

In addition, the algorithm can be potentially applied to other social networking sites such as Facebook. In Facebook, celebrities could be represented by the Facebook pages of the different celebrities and the followership links can be defined as the different user ``likes'' on these pages.

This paper provides an interesting alternative to detect communities by first specifying the interest of the community before detecting it, which may be used in the proposed research on top of the usual algorithms \cite{Lim:2012:1}.

\citeA{Bakillah:2014} sought to contribute to the field of extracting relevant information from social media by detecting geo-located communities in Twitter in disaster situations. The main disaster they focused on is the occurrence of typhoon Haiyan in the Philippines. 

Social graphs of Twitter users related to the focus are created by comparing Twitter's different interaction nodes like follow relations, mentions and tweet content. The fast-greedy optimization of modularity (FGM) clustering algorithm enhanced with semantic similarity is used in order to handle the complex social graphs created. Modularity measures the quality of divisions of a network into communities. By maximizing the modularity between the generated graph structure and a random graph structure, the optimal clustering results can be obtained. This modularity is expressed through the quality function $Q$:

\begin{equation}
	Q = \sum_{c = 1}^{n} \left [ \frac{l_c}{m} - \left (\frac{d_c}{2m} \right )^2 \right ]
\end{equation}where $n$ is the number of clusters, $m$ is the total number of edges, $l_c$ is the total number of edges joining the vertices of cluster $c$ and $d_c$ is the sum of the expected random graph degree of the vertices of $c$. To achieve the largest quality change ($\Delta$$Q$), communities are progressively merged. The usage of quality function $Q$ and the merging of communities are the core steps of the FGM algorithm. Now, the FGM algorithm was enhanced by integrating graph based and text-based (text similarity measure) models to balance the importance of shared content versus graph structure. This will be discussed more in detail in another section, Sentiment Analysis.

Together with FGM, the varied density-based spatial clustering of applications with noise spatial (VDBSCAN) clustering algorithm is used to get spatial communities at different time periods. This is done to divide thematic communities discussing same topics formed by the FGM algorithm into more meaningful sub-clusters. The discovery of geo-located communities could potentially help in identifying and locating incidents occurring during emergency situations.

The aim of the VDBSCAN spatial clustering algorithm is to find spatial clusters based on regions with higher density. The algorithm is based on two parameters:
\begin{enumerate}
	\item $r$: the value of the radius that will be used to select members of a cluster
	\item MinPts: minimal density to form a cluster.
\end{enumerate}
Let $D$ be the set of points corresponding to the geo-located tweets found in a given thematic community and that were issued during time period T.

The neighbour of point $p$ is expressed as:
\begin{equation}
	N(p) = \{ q \in D|dist(p,q) \le r \}
\end{equation}

A cluster is generated based on the following properties, called ‘density-reachable’ and ‘density-connected’:
\begin{enumerate}
	\item A point $q$ is directly density-reachable from a point $p$ if
	
	\begin{equation}
		q \in N(p,r) and |N(p,r)| \ge MinPts
	\end{equation}
	
	\item A point $q$ is density-reachable from a point $p$ if there exists a sequence of points $p_1$, …, $p_n$ where $p_1$ = $p$ and $p_n$ = $q$ such that $p_{i+1}$ is directly reachable from $p_i$, for all $i$.
	\item A point $q$ is density-connected to a point $p$ if there is a point $o$ such that $p$ and $q$ are density-reachable from $o$.
\end{enumerate}

A cluster is a non-empty subset of $D$ that satisfies the following properties:
\begin{enumerate}
	\item $\forall$ $p$, $q$, if $p$ $\in$ $C$ and $q$ is density - reachable from $p$, then $q$ $\in$ $C$ (maximality).
	\item $\forall$ $p$, $q$ $\in$ $C$, $p$ is density - connected to $q$ (connectivity).
\end{enumerate}

The parameters r and MinPts are optimized automatically based on the variation in density of the data set.

This research provides algorithms that could prove useful in getting the optimal clustering for detecting communities. It also gives an insight in considering the spatial and thematic properties of these communities \cite{Bakillah:2014}.

\section{Similarity Parameters}
This section outlines the basis/features/parameters used in detecting communities. It is divided into two subsections. The first subsection deals solely with sentiment analysis. The second subsection deals with other network and node parameters not related to sentiment analysis. 

\subsection{Sentiment Analysis}

\citeA{Zhang:2012} provided a formula to determine similarity in terms of text. 

\begin{equation}
	sim_{text}(i,j) = \frac{1}{\sqrt{D_{js}(i,j)}}
\end{equation}$D_{js}$ is the Jensen-Shannon Divergence between the two user’s topic probability distribution given by

\begin{equation}
	D_{js}(i,j) = \frac{D_{kl}(UT_i \mid\mid M) + D_{kl}(UT_j \mid\mid M)}{2}
\end{equation}where $M = \frac{UT_i + UT_j}{2}$ and $D_{kl}(P \mid\mid Q) = \sum_{i \in topics} P(i) \log{\frac{P(i)}{Q(i)}}$ and $UT_i$ is the probability distribution of user i for all topics. $UT_i[t]$ is the probability distribution for user i on topic t.

This research provides a metric to determine the similarity of two users in terms of post content, which can be used in the proposed research \cite{Zhang:2012}.

\citeA{Bakillah:2014} enhanced the FGM algorithm with a similarity measure. A threshold $T$ for text similarity is used to determine whether two communities are similar enough to increase the priority of merging them. 0.2-0.3 was used as the value of $T$. The cosine similarity measure is used to compute similarity between the communities' set of terms:

\begin{equation}
	Cosine similarity = \frac {A \cdot B}{||A|| ||B||} = \frac {\sum_{k = 1}^{l}(A_k \times B_k)}{\sqrt{\sum_{i = 1}^{n} (A_i)^2 \times \sum_{j = 1}^{m} (B_j)^2}}
\end{equation}

$A$ and $B$ represent the frequency of a term in the set of terms associated with the first and second community, respectively. The similarity value ranges from −1, meaning dissimilarity, to 1, meaning exact similarity.

This measure can be used as a means for getting the similarity between different communities’ set of words when merging similar communities will be relevant to the proposed research \cite{Bakillah:2014}.

\subsection{Other Parameters}
\citeA{Zhang:2012} provided a few formulas to determine similarity in terms of URL, hashtag, following, and retweeting similarity.

URL similarity is given by the same formula as text similarity in section 2.2.1, only using links instead of topics.

Hashtag similarity is given by 

\begin{equation}
	sim_{hashtag}(i,j) = \sum_{k=1}^n (1 - \left|{\frac{N_{ik}}{\left|{H_i}\right|} - \frac{N_{jk}}{\left|{H_j}\right|}}\right|)\frac{N_{ik} + N_{jk}}{\left|{H_i}\right| + \left|{H_k}\right|}
\end{equation}where $N_{ik}$ is the number of times user $v_i$ used the hashtag $k$ while $H_i$ is the total hashtags used by $v_i$.

Following similarity is given by 

\begin{equation}
	sim_{follow}(i,j) = \frac{c_{friend}}{\sqrt{\left|{Friend_i}\right|\left|{Friend_j}\right|}} + \frac{c_{follower}}{\sqrt{\left|{Follower_i}\right|\left|{Follower_j}\right|}}
\end{equation}

$\left|{Friend_i}\right|$ is the total number of users $v_i$ follows. $\left|{Follower_i}\right|$ is the total number of users that follow $v_i$. $c_{friend}$ represents the two users’ common friends. $c_{follower}$ represents the two users’ common followers.

Retweeting similarity is given by 

\begin{equation}
	sim_{retweet}(i,j) = \frac{c_{retweet}}{\sqrt{\left|{R_i}\right|\left|{R_j}\right|}} + \frac{n_{ij} + n_{ji}}{\left|{R_i}\right|\left|{R_j}\right|}
\end{equation}

$R_i$ is the number of users whom $v_i$ retweet. $c_{retweet}$ is the number of users both $v_i$ and $v_j$ retweet. $n_{ij}$ is the number of times $v_i$ retweeted $v_j$ and $n_{ji}$ is the inverse case. 

The aggregate similarity is now given by

\begin{equation}
	sim(i,j) = \gamma_t sim_{text} + \gamma_u sim_{url} + \gamma_h sim_{hashtag} + \gamma_f sim_{follow} + \gamma_r sim_{retweet}
\end{equation}

With $\gamma_{feature}$ determined in a process described in section 2.3.

This research gives formulas that can be used in the proposed research to measure similarity \cite{Zhang:2012}. 

\citeA{Bakillah:2014} created graphs with weighted edges with similarities based on Twitter's various interaction modes:
\begin {enumerate}
\item Graph based on mentions: A mention is a Twitter update that contains '@username' anywhere in the body of the tweet. It is used to reference another user. To build the graph, we create an edge between any pair of users $u_1$ and $u_2$ where $u_1$ issued a mention @$u_2$. The weight assigned to the edge increases with the number of mentions between $u_1$ and $u_2$ (nb\_mentions($u_1$, $u_2$) in the formula), but is normalized according to the total number of mentions in the graph and the total number of users:
\begin{equation}
	Weight_{u_1 u_2} = \frac{nb\_mentions(u_1, u_2)}{total\_nb\_mentions} \times total\_nb\_users
\end{equation}
\item Graph based on ‘follow relations’: Edges are established between users linked by a ‘follow relation’. The weight assigned to the edge normalizes the importance of the follow relation according to the number of follow relations by user 1 and the average number of follow relations by user in the network:
\begin{equation}
	Weight_{u_1 u_2} = \frac{nb\_follow\_rel(u_1)}{total\_nb\_follow\_rel} \times total\_nb\_users
\end{equation}
\item Graph based on shared URLs: Edges are created between any pair of users $u_1$ and $u_2$ who have shared the same URLs. The weight assigned to the edge increases with the number of URLs that $u_1$ and $u_2$ have shared (nb\_URLs($u_1$, $u_2$) in the formula), but is normalized according to the total number of shared URLs in the graph and the total number of users:
\begin{equation}
	Weight_{u_1 u_2} = \frac{nb\_URLs(u_1, u_2)}{total\_nb\_sharedURLs} \times total\_nb\_users
\end{equation}
\item Graph based on similar Tweet content: Words common to different users’ tweets are used to build the graph. The resulting graph represents shared interests between users, but not necessarily an explicit relation between them. The tweet content are categorized according to corresponding text elements in the tweet. Edges are created between users whose tweets contain some common text elements. Edge weights are assigned according to the number of common text elements in tweets of users $u_1$ and $u_2$ and normalized according to the total number of text elements that were extracted from tweets:
\begin{equation}
	Weight_{u_1 u_2} = \frac{nb\_common\_text\_el(u_1, u_2)}{total\_nb\_text\_el\_extracted} \times total\_nb\_users
\end{equation}

\end {enumerate}
This study presents alternative formulae that may be used in the proposed research \cite{Bakillah:2014}.

\citeA{Darmon:2015} aimed to present an approach to community detection that is multifaceted, focusing not only on structure-based communities, but on other types as well, namely activity-based, topic-based, and interaction based communities. Communities can be defined similarly or differently according to these types, so in order to come up with a more accurate and dynamic picture of a community, all types of communities, as well as the overlaps among these communities, should be taken into account. This study was done through the analysis of a Twitter dataset in order to assign representative weights for each community type. Activity-based communities were derived through the timing of users' tweets, topic-based communities were derived from hashtag similarities, and interaction-based communities were derived from retweets and mentions.

For activity-based communities, at time $t$, a user $u$ either posts a tweet $(X_{t}(u) = 1)$ or does not post a tweet $(X_{t}(u) = 0)$; this is how each user’s behavior is viewed. The flow of information from a user $u$ to a follower $f$ is represented by the estimated transfer entropy between their time series $X_{t}(u)$ and $X_{t}(f)$, which is computed through the following formula.

\begin{equation}
	W_{u \to f}^{TE(k)} = \tilde{TE_{X(u) \to X(f)}^{(k)}} 
\end{equation}

For topic-based communities, edges on the network of users and followers are weighted depending on the number of common hashtags between each user and follower pair. This weight is calculated using the following formula.

\begin{equation}
	W_{u \to f}^{HT} = \frac{\vec{h}(u)\vec{h}(f)}{\left|{\left|{\vec{h}(u)}\right|}\right| \left|{\left|{\vec{h}(f)}\right|}\right|}
\end{equation}

Interaction-based communities are defined by three weighting schemes. The first scheme considers the number of tweets follower $f$ retweeted from user $u$.

\begin{equation}
	W_{u \to f}^{R} = \frac{\text{\# retweets of $u$ by $f$}}{\text{\# total retweets made by $f$}}
\end{equation}

The second scheme considers the number of tweets wherein user $u$ mentions follower $f$.

\begin{equation}
	W_{u \to f}^{R} = \frac{\text{\# mentions of $u$ by $f$}}{\text{\# total mentions of $f$}}
\end{equation}

The third and final scheme takes the arithmetic mean of the mentions and retweets.

The study determined that the multifaceted approach to community detection could aid in better understanding the structure of online communities and in finding communities in social media that would otherwise be hidden.

The study provides an approach that may be considered as well as algorithms that may be used in the detection of communities in the proposed research \cite{Darmon:2015}.

\section{Community Evaluation Metrics}

\citeA{Zhang:2012} used the average number of mutual following links per user per community (FPUPC) to evaluate their communities. Based on this, appropriate weights for the aggregation were found by first performing their k-means clustering algorithm using only one feature similarity for each of the similarities and extracting the FPUPC. Afterwards, they gave each feature similarity a weight based on the following formula. 

\begin{equation}
	\gamma_{feature} = \frac{FPUPC_{feature}}{\sum_{f \in features} FPUPC_f}
\end{equation}

The number of clusters, k, used in the k-means clustering algorithm was also tweaked to get the maximum FPUPC. They concluded that they were successful in generating relatively accurate communities due to the incrementally increasing FPUPC after adjusting the weights.

This provides one possible evaluation metric for the proposed research, as well as a method to provide weights for the feature similarities that the proponents will eventually be using for community detection \cite{Zhang:2012}.